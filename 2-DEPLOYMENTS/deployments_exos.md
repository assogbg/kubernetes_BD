# Home made custom node js api - gomorra

Similar to POD exercise with custom images

## Create 2 deployments based on v3 of each custom image with 2 replicas
$ kubectl label deployments app1-test "canary=true"
$ kubectl get deployments -o wide --label-columns=canary

==> "-L" => --label-columns ==> != "-l" == "selector="
???works with deployements not with pods???
### create 2 pods based on the third image

to do so, either
- you create a yaml
- use "kubectl run command" - deprecated
> kubectl run --dry-run -o yaml savastano --image=pgolard/test-savastano:v3 --labels=app=savastano --replicas=2 --port=9999
```
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: savastano
  name: savastano
spec:
  replicas: 2
  selector:
    matchLabels:
      app: savastano
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: savastano
    spec:
      containers:
      - image: pgolard/test-savastano:v3
        name: savastano
        ports:
        - containerPort: 9999
        resources: {}
```

- use "create" command and update the yaml

> kubectl create deployment --dry-run -o yaml savastano --image=pgolard/test-savastano:v3
```
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: savastano
  name: savastano
spec:
  replicas: 1
  selector:
    matchLabels:
      app: savastano
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: savastano
    spec:
      containers:
      - image: pgolard/test-savastano:v3
        name: test-savastano
        resources: {}
```




### analyze your pods

> kubectl get pods --show-labels -o wide

```
NAME                         READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES   LABELS
ciro-59ffbfbf4f-5829f        1/1     Running   0          42s   172.17.0.6   minikube   <none>           <none>            app=ciro,pod-template-hash=59ffbfbf4f
ciro-59ffbfbf4f-mmhlw        1/1     Running   0          42s   172.17.0.8   minikube   <none>           <none>            app=ciro,pod-template-hash=59ffbfbf4f
savastano-79b985b446-4zzxj   1/1     Running   0          74s   172.17.0.3   minikube   <none>           <none>            app=savastano,pod-template-hash=79b985b446
savastano-79b985b446-rp8xd   1/1     Running   0          74s   172.17.0.2   minikube   <none>           <none>            app=savastano,pod-template-hash=79b985b446
```
PLEASE PAY ATTENTION TO THE LABELS. You actually find two labels for each pods :
- 1 is specified in your deployment specs (app: savastano or app:ciro)
- 2nd (pod-template-hash) is generated by the replicaset controller (encapsulated in your deployment)

> kubectl get deploy,rs
```
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/ciro        2/2     2            2           12m
deployment.apps/savastano   2/2     2            2           13m

NAME                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/ciro-59ffbfbf4f        2         2         2       12m
replicaset.apps/savastano-79b985b446   2         2         2       13m
```


### check the rollout history

> kubectl rollout history deployment ciro
```
deployment.apps/ciro
REVISION  CHANGE-CAUSE
1         <none>
```

> kubectl rollout history deployment ciro --revision=1
```
deployment.apps/ciro with revision #1
Pod Template:
  Labels:	app=ciro
	pod-template-hash=59ffbfbf4f
  Containers:
   ciro:
    Image:	pgolard/test-ciro:v3
    Port:	7777/TCP
    Host Port:	0/TCP
    Environment:	<none>
    Mounts:	<none>
  Volumes:	<none>
```

We see that it keeps tracks of the revision (the changes you've made to your deployment).

If we go further into that, we see that there is a limit to the version that your deployment is going to keep as revision in its history.
This is actually the main added value of a deployment : you can easily rollout/rollbacks.
In addition to that, when it processes a rollout/rollback, a new replicaset will be created and it will progressively creates new pods before the previous replicaset terminates its pods.
Therefore, there won't be any downtime.

> kubectl rollout history deployment ciro -o yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2020-04-30T02:30:56Z"
  generation: 1
  labels:
    app: ciro
  name: ciro
  namespace: default
  resourceVersion: "213539"
  selfLink: /apis/apps/v1/namespaces/default/deployments/ciro
  uid: ad934a2b-6d3b-4d94-80cc-5cbcf36555ef
spec:
  progressDeadlineSeconds: 600
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: ciro
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: ciro
    spec:
      containers:
      - image: pgolard/test-ciro:v3
        imagePullPolicy: IfNotPresent
        name: ciro
        ports:
        - containerPort: 7777
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2020-04-30T02:30:59Z"
    lastUpdateTime: "2020-04-30T02:30:59Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2020-04-30T02:30:56Z"
    lastUpdateTime: "2020-04-30T02:30:59Z"
    message: ReplicaSet "ciro-59ffbfbf4f" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 2
  replicas: 2
  updatedReplicas: 2
```

## modifiy the deployment - set the image to v4

> kubectl set image deployment ciro ciro=pgolard/test-ciro:v4 --record

> kubectl rollout status deployment ciro
```
deployment "ciro" successfully rolled out
```

> kubectl rollout history deployment ciro
```
deployment.apps/ciro
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment ciro ciro=pgolard/test-ciro:v4 --record=true
```

### check pods and replicasets
> kubectl get pods --show-labels -o wide

```
NAME                         READY   STATUS    RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES   LABELS
ciro-6ffb8758f6-qblzg        1/1     Running   0          104s   172.17.0.9    minikube   <none>           <none>            app=ciro,pod-template-hash=6ffb8758f6
ciro-6ffb8758f6-vgqzr        1/1     Running   0          100s   172.17.0.10   minikube   <none>           <none>            app=ciro,pod-template-hash=6ffb8758f6
savastano-79b985b446-4zzxj   1/1     Running   0          32m    172.17.0.3    minikube   <none>           <none>            app=savastano,pod-template-hash=79b985b446
savastano-79b985b446-rp8xd   1/1     Running   0          32m    172.17.0.2    minikube   <none>           <none>            app=savastano,pod-template-hash=79b985b446
```
> kubectl get deploy,rs
```
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/ciro        2/2     2            2           33m
deployment.apps/savastano   2/2     2            2           34m

NAME                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/ciro-59ffbfbf4f        0         0         0       33m
replicaset.apps/ciro-6ffb8758f6        2         2         2       3m46s
replicaset.apps/savastano-79b985b446   2         2         2       34m
```

Please PAY ATTENTION TO replicasets:
we now see that ciro deployment has had 2 different replicasets
- the initial one with desired and current nb of replicas set to zero
- the new one with desired and current nb of replicas set to 2

This also illustrates the role of the replicaset controller that makes sure current and desired state are equal.
## rollback
Let's say that we now want to rollback our last upgrade and move back to the first revision:
> kubectl rollout undo deployment ciro --to-revision=1

> kubectl rollout history deployment ciro
```
deployment.apps/ciro
REVISION  CHANGE-CAUSE
2         kubectl set image deployment ciro ciro=pgolard/test-ciro:v4 --record=true
3         <none>
```
>kubectl rollout history deployment ciro --revision=3
```
deployment.apps/ciro with revision #3
Pod Template:
  Labels:	app=ciro
	pod-template-hash=59ffbfbf4f
  Containers:
   ciro:
    Image:	pgolard/test-ciro:v3
    Port:	7777/TCP
    Host Port:	0/TCP
    Environment:	<none>
    Mounts:	<none>
  Volumes:	<none>
```
